---
title: "cord_c18data_cleaning_attempt"
author: "Alana Ferris"
date: "2024-02-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Reading in the plasma c18 data
```{r}
c18_ftr_table =
  read.table("data/RAW_c18neg_featuretable.txt", header = TRUE)
  
c18_annotation =
  read.csv("data/c18neg_annotation_Stage5.csv", header = TRUE) %>% 
  janitor::clean_names()

c18_key =
  read.table("data/plasma_mapping_c18neg.txt", header = TRUE) %>% 
  janitor::clean_names()
```

## C18_key: separating info in sample_id column to create a new, clean, sample id called "sid"
```{r}
c18_key <-
  c18_key %>%
  mutate(file_name = paste0(file_name, ".mzXML"),
         type = ifelse(grepl("nist", sample_id), "NIST",
                     ifelse(grepl("Plasma", sample_id), "FS", "QC")),
         sample_id = str_remove(sample_id,"Plasma_"),
         sid = str_remove(sample_id,"_[^_]+$")) %>%
  filter(file_name%in%names(c18_ftr_table))
```

## c18_ftr_table: renaming sample ids to the clean sid made in c18_key
```{r}
c18_ftr_table <- 
  c18_ftr_table %>%
  rename_at(vars(c18_key$file_name), ~ c18_key$sid)
```

## c18_annotation: matching annotations
AKA names of compounds (based on mz and time) identified in their library or public databases to our features detected in each sample id
```{r}
c18_annotation <-
  c18_annotation %>%
  mutate(time = round(time, 1),
         mz = round(mz, 5),
        mz_time = paste0(mz, "_", time))

c18_ftr_table <-
  c18_ftr_table %>%
  mutate(time = round(time, 1),
         mz = round(mz, 5),
        mz_time = paste0(mz, "_", time)) %>%
  relocate(mz_time) %>% #use relocate to change column positions, like select()
  left_join(c18_annotation %>% 
              select(mz_time, name, annotation_confidence_score)) %>%
  relocate(name, annotation_confidence_score) %>%
  distinct(mz_time, .keep_all = T) #10456

c18_ftr_table %>% 
  distinct(mz_time)


#check if any NAs - none
c18_ftr_table %>%
  select(2:ncol(.)) %>%  # replace to your needs
  summarise_all(funs(sum(is.na(.))))
#7707 NAs in annotation_confidence_score but none otherwise 
```

# c18_ftr_table: proportion of missing features
```{r}
missing_output <- c18_ftr_table %>%
    mutate(missing = rowSums(across(starts_with("S2"), `%in%`, 0))) %>%
    rowwise() %>%
    mutate(prop = missing/sum(grepl("S2",names(c18_ftr_table)), na.rm = TRUE)) %>%
    select(mz_time, missing, prop)

missing_output %>%
  ggplot(., aes(x = prop)) +
  geom_histogram()

#ggsave("Prop_missing.pdf")
# come back to save this output?

met_data_cutoff <- 
  function(cutoffpercent, columnpattern, outputfile){
  met_data_output <- c18_ftr_table %>%
    mutate(missing = rowSums(across(starts_with(columnpattern),`%in%`,0))) %>%
    rowwise() %>%
    mutate(prop = missing/sum(grepl("S2",names(c18_ftr_table)), na.rm = TRUE))%>%
    filter(prop < cutoffpercent) %>%
    select(-c(missing, prop)) 
  write.csv(met_data_output, outputfile, row.names = FALSE) #why are we saying row names false here? --> can try running without it to see what happens 
  return (met_data_output)
}

met_data_50cutoff <- 
  met_data_cutoff(0.5,"S2","c18_ftr_table_50cutoff.csv") #9872 obs
```

## c18_ftr_table: filter out features where RSD of QC > 30%
exactly like it sounds, in this step we are filtering out data that is not "high quality" 
```{r}
rsd <- 
  met_data_50cutoff %>%
  rowwise() %>%
  mutate(sd = sd(c_across(starts_with("q3",ignore.case = F))),
         mean = mean(c_across(starts_with("q3",ignore.case = F))),
         rsd = 100*(sd/mean),
         threshold = ifelse(rsd > 30,"Yes","No")) %>%
  select(mz_time, sd, mean, rsd, threshold) #this will take a minute to run

rsd %>%
  ggplot(., aes(x = rsd)) +
  geom_histogram() +
  geom_vline(xintercept = 30, colour="red", linetype = "longdash")

#ggsave("normalized_data/placenta_C18/Plots/RSD.pdf") <- come back to this 

summary(factor(rsd$threshold)) #3513 below threshold; but i also see i have 666 NAs...why --> maya asking Xin about this in meeting bc if qc arent actually technical replicates not serving purpose we are using them 

met_data_rsd_cutoff <- function(cutoffpercent, columnpattern, outputfile){
  met_data_output <- met_data_50cutoff %>%
    rowwise() %>%
    mutate(sd = sd(c_across(starts_with("q3",ignore.case = F))),
         mean = mean(c_across(starts_with("q3",ignore.case = F))),
         rsd = 100*(sd/mean)) %>%
    filter(rsd < cutoffpercent) %>%
    select(-c(sd, mean, rsd)) 
  write.csv(met_data_output, outputfile, row.names = FALSE)
  return (met_data_output)
}

met_data_50_rsd_cutoff <- met_data_rsd_cutoff(30,"S2","C18_50cutoff_RSD.csv") #3513 don't pass...this takes a minute to run

```

# c18_ftr_table: number of features per sample 
```{r}
met_data_50_rsd_cutoff %>%
  select(mz_time, starts_with('S2')) %>%
  pivot_longer(names_to = "sample", values_to = "value", -c(mz_time)) %>%
  mutate(sample = ifelse(value == 0, NA, sample)) %>% 
  drop_na(sample) %>%
  ggplot(., aes(sample)) +
  geom_bar() +
  theme_bw() +
  ylab("Number of features") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1)) #there are about equal number of features per sample, no obvious outliers

#ggsave("plasma_c18_analytes_per_sample.pdf")
```

# c18_ftr_table: missing value imputation (mice)
```{r}
library(mice)
library(dplyr)
library(patchwork)

#replacing all 0 occurrences with NAs so that I can impute values for the NAs
met_data_replace <-
  met_data_50_rsd_cutoff %>% 
  ungroup() %>% 
  mutate(across(starts_with("S2"), ~ ifelse(. == "0", "", .))) %>%
  mutate(across(starts_with("S2"), as.numeric))

summary(met_data_replace)

# now trying 2 different imputation methods. tried pmm but it wasn't working so I gave up.
met_data_imputed <- 
  data.frame(
  original = met_data_replace$S2036,
  #imputed_pmm = complete(mice(met_data_replace, method = "pmm"))$S2036,
  imputed_cart = complete(mice(met_data_replace, method = "cart"))$S2036,
  imputed_lasso = complete(mice(met_data_replace, method = "lasso.norm"))$S2036
)

met_data_imputed
summary(met_data_imputed)

#lasso imputing negative values which i don't think are possible

h1 <- ggplot(met_data_imputed, aes(x = original)) +
  geom_histogram(fill = "#ad1538", color = "#000000", position = "identity") +
  ggtitle("Original distribution") +
  theme_classic()
h2 <- ggplot(met_data_imputed, aes(x = imputed_cart)) +
  geom_histogram(fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("CART-imputed distribution") +
  theme_classic()
h3 <- ggplot(met_data_imputed, aes(x = imputed_lasso)) +
  geom_histogram(fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("LASSO-imputed distribution") +
  theme_classic()

h1 + h2 + h3 + plot_layout(nrow = 2, ncol = 2)

# since lasso didn't seem the best, going to impute using cart
#whole data imputation...

#ctrl + shift + c to gray out a chunk of code 
# only_samples =
#   met_data_replace %>% 
#   select(-name, -annotation_confidence_score, -mz_time, -mz, -time, )

set.seed(12345)

#need to discuss what should or shouldn't be used as a predictor? --> restrict only to sample columns, not qc 

met_data_impute_2 <-
  mice(met_data_replace, m = 5, method = "cart", seed = 1000)
# last time i did imputation i did m=10 but just to save time i am making it 5 for now, i also think that is the standard 

#get imputations back to make a completed data 
cart_complete_met_data <- complete(met_data_impute_2,1)

#plot distribution of original and imputed data 
densityplot(met_data_impute_2) #the original and imputed distribution look similar but tends to overestimate 
# The imputed curve (in red) plots the density of the mean imputation over the m datasets. That is, for each cell that is missing in the variable, the diagnostic will find the mean of that cell across each of the m datasets and use that value for the density plot. The black distributions are the those of the observed data. When variables are completely observed, their densities are plotted in blue

stripplot(met_data_impute_2, pch = 20, cex = 1.2) #this isnt showing anything idk what its supposed to be showing

#for more info on imputing https://cran.r-project.org/web/packages/Amelia/vignettes/diagnostics.html

# before imputation shouldnt have negative values, but after scaling yes 
```

### c_18_ftr_table: log transform data
```{r}
met_norm_log2 <- 
  cart_complete_met_data %>%
  mutate(across(starts_with("S2"), log2))
```

### pareto scaling
A form of normalization, scale it to the mean and sd of each feature by subtracting the mean from all samples and dividing by standard deviation 
```{r pareto, message=FALSE, warning=FALSE}

paretoscale <- function(z){
  rowMean <- apply(z, 1, mean)                         # row means
  rowMAD <- apply(z, 1, mad)                           # row standard deviation
  rowSqrtSD <- sqrt(rowMAD)                            # sqrt(SD)
  cv <- sweep(z, 1, rowMean, "-")                      # mean center
  cv <- sweep(cv, 1, rowSqrtSD, "/")                  # divide by sqrt(SD)
  return(cv) 
}

met_log_pareto <- paretoscale(met_norm_log2[,c(grepl("S2",names(met_norm_log2)))]) 
met_log_pareto <- cbind(met_norm_log2[,!names(met_norm_log2)%in%names(met_log_pareto)],met_log_pareto)

met_log_pareto<-met_log_pareto%>%
  select(name, annotation_confidence_score, mz_time, starts_with("S2"))

#test<-met_norm_log2%>%
#  mutate(across(starts_with("S2"),paretoscale))

saveRDS(met_log_pareto,"plasma_C18_50cutoff_RSD_norm_scaled.rds")
```

### most abundant metabolites
```{r}
top.abundance <- met_log_pareto %>%
  drop_na(name) %>%
  filter(annotation_confidence_score > 0) %>%
  pivot_longer(names_to = "Sample", 
               values_to = "value", 
               -c(mz_time, name, annotation_confidence_score)) %>%
  group_by(mz_time) %>% 
  mutate(median = median(value)) %>%
  distinct(mz_time, median, .keep_all = T) %>%
  ungroup() %>%
  slice_max(median, n=20 ) %>%
  select(mz_time, name, annotation_confidence_score, median)

met_log_pareto %>%
  filter(mz_time%in%top.abundance$mz_time) %>%
  pivot_longer(names_to = "Sample", values_to = "value", -c(mz_time, name, annotation_confidence_score)) %>%
  ggplot(., aes(x = reorder(name, value, FUN = median), y = value)) +
  geom_boxplot() +
  geom_jitter(shape = 16, alpha = 0.5, position = position_jitter(0.2)) +
  theme_bw() +
  ylab("Metabolite abundance") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1,hjust = 1))

ggsave("C18_abundant_annotated.pdf")
```


